
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{A1-Iyer-Chen}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Foundations of Data Mining: Assignment
1}\label{foundations-of-data-mining-assignment-1}

Suraj Iyer (0866094) Simin Chen (0842556)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{from} \PY{n+nn}{preamble} \PY{k}{import} \PY{o}{*}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{savefig.dpi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{100}
        \PY{n}{InteractiveShell}\PY{o}{.}\PY{n}{ast\PYZus{}node\PYZus{}interactivity} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{all}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}

    \subsection{Handwritten digit recognition (5 points,
1+2+2)}\label{handwritten-digit-recognition-5-points-122}

The \href{https://www.openml.org/d/554}{MNIST dataset} contains 70,000
images of handwritten digits (0-9) represented by 28 by 28 pixel values.
We can easily download it from OpenML and visualize one of the examples:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} This is a temporary read\PYZhy{}only OpenML key. Replace with your own key later. }
        \PY{n}{oml}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{apikey} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{11e82c8d91c5abece86f424369c71590}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{mnist\PYZus{}data} \PY{o}{=} \PY{n}{oml}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{l+m+mi}{554}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Download MNIST data}
        \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{mnist\PYZus{}data}\PY{o}{.}\PY{n}{get\PYZus{}data}\PY{p}{(}\PY{n}{target}\PY{o}{=}\PY{n}{mnist\PYZus{}data}\PY{o}{.}\PY{n}{default\PYZus{}target\PYZus{}attribute}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Get the predictors X and the labels y}
        \PY{c+c1}{\PYZsh{} X, y = X[:2000], y[:2000]  \PYZsh{} TODO: REMOVE THIS LATER}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{gray\PYZus{}r}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Take the first example, reshape to a 28x28 image and plot}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class label:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Print the correct class label}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:} <matplotlib.image.AxesImage at 0x23bacb44a58>
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Class label: 5

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_4_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{itemize}
\tightlist
\item
  Evaluate a k-Nearest Neighbor classifier with its default settings.

  \begin{itemize}
  \tightlist
  \item
    Use the first 60,000 examples as the training set and the last
    10,000 as the test set
  \item
    What is the predictive accuracy?
  \item
    Find a few misclassifications, and plot them together with the true
    labels (as above). Are these images really hard to classify?
  \end{itemize}
\item
  Optimize the value for the number of neighbors \(k\) (keep \(k\)
  \textless{} 50) on a stratified subsample (e.g. 10\%) of the data

  \begin{itemize}
  \tightlist
  \item
    Use 10-fold crossvalidation and plot \(k\) against the
    misclassification rate. Which value of \(k\) should you pick?
  \item
    Do the same but with 100 bootstrapping repeats. Are the results
    different? Explain.
  \end{itemize}
\item
  Compare kNN against the linear classification models that we have
  covered in the course (logistic regression and linear SVMs).

  \begin{itemize}
  \tightlist
  \item
    First use the default hyperparameter settings.
  \item
    Next, optimize for the degree of regularization (\(C\)) and choice
    of penalty (L1/L2). Again, plot the accuracy while increasing the
    degree of regularization for different penalties. Interpret the
    results.
  \item
    Report is the optimal performance. Can you get better results than
    kNN?
  \end{itemize}
\end{itemize}

Report all results clearly and interpret the results.\\
Note: while prototyping/bugfixing, you can speed up experiments by
taking a smaller sample of the data, but report your results as
indicated above.

    \paragraph{Evaluate a k-Nearest Neighbor classifier with its default
settings}\label{evaluate-a-k-nearest-neighbor-classifier-with-its-default-settings}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{MinMaxScaler}
         
         \PY{n}{min\PYZus{}max\PYZus{}scaler} \PY{o}{=} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}
         \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{60000}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} X\PYZus{}train, X\PYZus{}test = min\PYZus{}max\PYZus{}scaler.fit\PYZus{}transform(X\PYZus{}train), min\PYZus{}max\PYZus{}scaler.fit\PYZus{}transform(X\PYZus{}test)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{| y\PYZus{}train:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{| X\PYZus{}test:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{| y\PYZus{}test:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Predictive accuracy}
         \PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KNN score (predictive accuracy): }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Finding a few misclassifications}
         \PY{n}{X\PYZus{}misc} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)} \PY{k}{if} \PY{n}{y} \PY{o}{!=} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
         \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{y\PYZus{}true} \PY{o+ow}{in} \PY{n}{X\PYZus{}misc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{gray\PYZus{}r}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predicted class label:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{, True class label:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
X\_train: (60000, 784) | y\_train: (60000,) | X\_test: (10000, 784) | y\_test: (10000,)
KNN score (predictive accuracy): 0.968800

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.image.AxesImage at 0x2852cb02b38>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_7_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class label: 0 , True class label: 4

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.image.AxesImage at 0x2852c44b208>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_7_5.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class label: 9 , True class label: 4

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.image.AxesImage at 0x2852c841908>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_7_8.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class label: 1 , True class label: 3

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.image.AxesImage at 0x2852c8a74a8>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_7_11.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class label: 8 , True class label: 9

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.image.AxesImage at 0x2852cb4c390>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_7_14.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Predicted class label: 6 , True class label: 4

    \end{Verbatim}

    Some of the images are indeed quite difficult to recognize even by
human. For example, the first misclassified 4 is not recognizable as any
number, even as its predicted value. The second misclassified 4 looks
quite like 9.

    \paragraph{\texorpdfstring{Optimize the value for the number of
neighbors \(k\) (keep \(k\) \textless{} 50) on a stratified subsample
(e.g. 10\%) of the
data}{Optimize the value for the number of neighbors k (keep k \textless{} 50) on a stratified subsample (e.g. 10\%) of the data}}\label{optimize-the-value-for-the-number-of-neighbors-k-keep-k-50-on-a-stratified-subsample-e.g.-10-of-the-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}
        
        \PY{c+c1}{\PYZsh{} Build a list of the training and test scores for increasing k}
        \PY{n}{misc\PYZus{}rate} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{k} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Get 10\PYZpc{} of the data}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{n\PYZus{}neighbors} \PY{o+ow}{in} \PY{n}{k}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} build the model}
            \PY{n}{clf} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{n\PYZus{}neighbors}\PY{p}{)}
            \PY{n}{scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{misc\PYZus{}rate}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mf}{1.}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} plot the data}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{12.}\PY{p}{,} \PY{l+m+mf}{10.}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{misc\PYZus{}rate}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{misc rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{k}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{misc\PYZus{}rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{6.}\PY{p}{,} \PY{l+m+mf}{4.}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor} }]:} [<matplotlib.lines.Line2D at 0x1e392689a58>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_10_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    We should take \(k = 1\) because it has the least misclassification
rate.

    \paragraph{Doing the same as above but with 100 bootstrapping
repeats.}\label{doing-the-same-as-above-but-with-100-bootstrapping-repeats.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{,} \PY{n}{StratifiedShuffleSplit}
         \PY{n}{sss} \PY{o}{=} \PY{n}{StratifiedShuffleSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.66}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Build a list of the training and test scores for increasing k}
         \PY{n}{misc\PYZus{}rate} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{k} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get 10\PYZpc{} of the data}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{05}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{n\PYZus{}neighbors} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} build the model}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{clf} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{n\PYZus{}neighbors}\PY{p}{)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{sss}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{misc\PYZus{}rate}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the data}
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{12.}\PY{p}{,} \PY{l+m+mf}{10.}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{misc\PYZus{}rate}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{misc rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{k}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{misc\PYZus{}rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{6.}\PY{p}{,} \PY{l+m+mf}{4.}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} [<matplotlib.lines.Line2D at 0x24cb31e6e10>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_13_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    There is more variation in the result without bootstrapping repeat,
while the misclassification rate grows more smoothly as the number of
neighbors increases with bootstrapping repeat. This is because there is
no guarantee for the randomness of the training data. By taking 100
times bootstrapping, the cross-validation score is averaged across
multiple random permutation of the training data. Therefore the result
is more stable.

    \paragraph{Compare kNN against the linear classification models that we
have covered in the course (logistic regression and linear SVMs) using
the default hyperparameter
settings.}\label{compare-knn-against-the-linear-classification-models-that-we-have-covered-in-the-course-logistic-regression-and-linear-svms-using-the-default-hyperparameter-settings.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}
         
         \PY{c+c1}{\PYZsh{} Get 10\PYZpc{} of the data}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Build the models}
         \PY{n}{knn}\PY{p}{,} \PY{n}{logistic}\PY{p}{,} \PY{n}{svc} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{clf} \PY{o+ow}{in} \PY{p}{[}\PY{n}{knn}\PY{p}{,} \PY{n}{logistic}\PY{p}{,} \PY{n}{svc}\PY{p}{]}\PY{p}{:}
             \PY{n}{scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ Mean cross\PYZhy{}validation score: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
KNeighborsClassifier Mean cross-validation score: 0.939142
LogisticRegression Mean cross-validation score: 0.827266
LinearSVC Mean cross-validation score: 0.834135

    \end{Verbatim}

    \paragraph{\texorpdfstring{Optimizing for the degree of regularization
(\(C\)) and choice of penalty
(L1/L2).}{Optimizing for the degree of regularization (C) and choice of penalty (L1/L2).}}\label{optimizing-for-the-degree-of-regularization-c-and-choice-of-penalty-l1l2.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{GridSearchCV}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{66}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Build the models}
         \PY{n}{logistic\PYZus{}l1}\PY{p}{,} \PY{n}{logistic\PYZus{}l2}\PY{p}{,} \PY{n}{svc} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]} \PY{p}{\PYZcb{}}
         
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{clf} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{p}{[}\PY{n}{logistic\PYZus{}l1}\PY{p}{,} \PY{n}{logistic\PYZus{}l2}\PY{p}{,} \PY{n}{svc}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{score} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ score: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,} \PY{n}{score}\PY{p}{)}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
0 LogisticRegression score: 0.873109

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} [<matplotlib.lines.Line2D at 0x23badb9fc18>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_18_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
1 LogisticRegression score: 0.850420

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} [<matplotlib.lines.Line2D at 0x23badccf048>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_18_5.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
2 
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{LinearSVC}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{66}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Build the model}
         \PY{n}{svc} \PY{o}{=} \PY{n}{LinearSVC}\PY{p}{(}\PY{p}{)}
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]} \PY{p}{\PYZcb{}}
         \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{svc}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ score: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}\PY{p}{,} \PY{n}{score}\PY{p}{)}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
LinearSVC score: 0.830252

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} [<matplotlib.lines.Line2D at 0x23badc43ba8>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_19_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the graphs, the decreasing trend of the accuracy with increasing
'C' (lower regularization) value is clearly evident for logistic
regression with both L1 and L2 penalty. The curves are very similar and
shows that the choice of penalty does not really have any effect on this
dataset. This trend does not hold the same way for the linear SVM model
where it decreases first having global minimum for C=0.1 and then
increases again till C=1 and then decreases again. This shows it has
more sensitivty towards regularization than the logistic regression
models for this dataset. Based on the accuracies we have seen for the
linear models, the KNN classifier performs better both using default
values as well as tuned values.

    \subsection{Model selection (4 points
(2+2))}\label{model-selection-4-points-22}

Study how RandomForest hyperparameters interact on the Ionosphere
dataset (OpenML ID 59).

\begin{itemize}
\tightlist
\item
  Optimize a RandomForest, varying both \(n\_estimators\) and
  \(max\_features\) at the same time. Use a nested cross-validation and
  a grid search (or random search) over the possible values, and measure
  the AUC. Explore how fine-grained this grid/random search can be,
  given your computational resources. What is the optimal AUC
  performance you find?
\item
  Again, vary both hyperparameters, but this time use a grid search and
  visualize the results as a plot (heatmap)
  \(n\_estimators \times max\_features \rightarrow AUC\) with AUC
  visualized as the color of the data point. Try to make the grid as
  fine as possible. Interpret the results. Can you explain your
  observations? What did you learn about tuning RandomForests?
\end{itemize}

Hint: Running this experiment can take a while, so start early and use a
feasible grid/random search. Start with a coarse grid or few random
search iterations. Hint: Use a log scale (1,2,4,8,16,...) for
\(n\_estimators\). Vary \(max\_features\) linearly between 1 and the
total number of features. Note that, if you give \(max\_features\) a
float value, it will use it as
\href{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}{the
percentage of the total number of features}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         
         \PY{n}{ionosphere} \PY{o}{=} \PY{n}{oml}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{l+m+mi}{59}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Download Ionosphere data}
         \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{ionosphere}\PY{o}{.}\PY{n}{get\PYZus{}data}\PY{p}{(}\PY{n}{target}\PY{o}{=}\PY{n}{ionosphere}\PY{o}{.}\PY{n}{default\PYZus{}target\PYZus{}attribute}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Get the predictors X and the labels y}
         \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} ((351, 34), (351,))
\end{Verbatim}
        
    \paragraph{\texorpdfstring{Optimize a RandomForest, varying both
\(n\_estimators\) and \(max\_features\) at the same time using a nested
cross-validation and a randomized
search.}{Optimize a RandomForest, varying both n\textbackslash{}\_estimators and max\textbackslash{}\_features at the same time using a nested cross-validation and a randomized search.}}\label{optimize-a-randomforest-varying-both-n_estimators-and-max_features-at-the-same-time-using-a-nested-cross-validation-and-a-randomized-search.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{,} \PY{n}{RandomizedSearchCV}
        
        \PY{n}{clf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}
        \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{]}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
        \PY{p}{\PYZcb{}}
        \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{roc\PYZus{}auc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{80}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{roc\PYZus{}auc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scores:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean cross\PYZhy{}validation score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{scores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Scores: [ 0.98   0.969  0.933  1.     0.992]
Mean cross-validation score: 0.975001709402

    \end{Verbatim}

    When \(n\_iter = 20\), we are able to get the best results within
acceptable time limit. We tested with higher iterations but they gave
negligable improvements to the performance but at the same time took way
longer to train. So, we decided to stick with 20 iterations. The optimal
AUC score we found was 1.

    \paragraph{Again, vary both hyperparameters, but this time use a grid
search and visualize the results as a plot
(heatmap)}\label{again-vary-both-hyperparameters-but-this-time-use-a-grid-search-and-visualize-the-results-as-a-plot-heatmap}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{GridSearchCV}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{66}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
         \PY{p}{\PYZcb{}}
         \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{roc\PYZus{}auc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{results} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}
         \PY{n}{scores} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Display the heatmap}
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{12.}\PY{p}{,} \PY{l+m+mf}{10.}\PY{p}{)}
         \PY{n}{mglearn}\PY{o}{.}\PY{n}{tools}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{xticklabels}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                               \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{yticklabels}\PY{o}{=}\PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{viridis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{6.}\PY{p}{,} \PY{l+m+mf}{4.}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} <matplotlib.collections.PolyCollection at 0x1e3e20210b8>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_27_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    The heat map shows that for this specific data set, the more trees in
the RandomForest model the better. With \(n\_estimators \geq 16\), the
score does not change much anymore. When the number of trees is fixed,
the less \(max\_features\), the better for when
\(n\_estimators \geq 16\) while for \(n\_estimators \leq 8\), more
optimal results come when \(max\_features\) is somewhere in between.
Normally it would be expected that with more number of features to
choose from, we should get better results but obviously based on the
results, this is not the case. Apparently, by introducing more number of
features, the number of differences between trees generated reduces,
thereby decreasing diversity of the forest. The lesser the diversity,
the more chances of incorrect predictions if the model is wrong. On the
other hand, when there are not enough trees, the number of features
plays relatively greater role.

    \subsection{Decision tree heuristics (1
point)}\label{decision-tree-heuristics-1-point}

Consider the toy training set created below. It predicts whether your
date agrees to go out with you depending on the weather.

Learn a decision tree:

\begin{itemize}
\tightlist
\item
  Implement functions to calculate entropy and information gain
\item
  What is the class entropy for the entire dataset? What is the
  information gain when you split the data using the \emph{Water}
  feature?
\item
  Implement a basic decision tree:

  \begin{itemize}
  \tightlist
  \item
    Select a feature to split on according to its information gain. If
    multiple features are equally good, select the leftmost one.
  \item
    Split the data and repeat until the tree is complete.
  \item
    Print out the results (nodes and splits).
  \end{itemize}
\item
  Now train a scikit-learn decision tree on the same data. Do you get
  the same result? Explain.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sky}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sunny}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sunny}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rainy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sunny}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sunny}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AirTemp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Humidity}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{high}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{high}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{high}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wind}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{strong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{strong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{strong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{strong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weak}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Water}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cool}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Forecast}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{change}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{change}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Date?}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                            \PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sky}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AirTemp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Humidity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Fix column ordering}
         \PY{n}{df}  \PY{c+c1}{\PYZsh{} print}
         
         \PY{c+c1}{\PYZsh{} One\PYZhy{}hot encode}
         \PY{n}{cols\PYZus{}to\PYZus{}transform} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sky}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AirTemp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Humidity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{df1} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{cols\PYZus{}to\PYZus{}transform}\PY{p}{)}
         \PY{n}{df1}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sky\PYZus{}rainy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AirTemp\PYZus{}cold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Humidity\PYZus{}high}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wind\PYZus{}strong}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water\PYZus{}cool}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast\PYZus{}change}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}no}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{df1}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:}      Sky AirTemp Humidity    Wind Water Forecast Date?
         0  sunny    warm   normal  strong  warm     same   yes
         1  sunny    warm     high  strong  warm     same   yes
         2  rainy    warm     high  strong  cool   change    no
         3  sunny    cold     high  strong  warm   change   yes
         4  sunny    warm   normal    weak  warm     same    no
\end{Verbatim}
        
            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:}    Sky\_sunny  AirTemp\_warm  Humidity\_normal  Wind\_weak  Water\_warm  \textbackslash{}
         0          1             1                1          0           1   
         1          1             1                0          0           1   
         2          0             1                0          0           0   
         3          1             0                0          0           1   
         4          1             1                1          1           1   
         
            Forecast\_same  Date?\_yes  
         0              1          1  
         1              1          1  
         2              0          0  
         3              0          1  
         4              1          0  
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Complete these functions first}
         \PY{c+c1}{\PYZsh{} pos and neg are the number of positive and negative samples in a node}
         \PY{k}{def} \PY{n+nf}{entropy}\PY{p}{(}\PY{n}{pos}\PY{p}{,} \PY{n}{neg}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{pos} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{neg} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                 \PY{k}{return} \PY{l+m+mi}{0}
             \PY{n}{p} \PY{o}{=} \PY{n}{pos}\PY{o}{/}\PY{p}{(}\PY{n}{pos}\PY{o}{+}\PY{n}{neg}\PY{p}{)}
             \PY{k}{return} \PY{o}{\PYZhy{}} \PY{n}{p}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{n}{p}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{p}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{p}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} pos1 and pos2 are the number of positive examples in each branch after the split. }
         \PY{c+c1}{\PYZsh{} Same for neg1 and neg2 }
         \PY{k}{def} \PY{n+nf}{info\PYZus{}gain}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{neg1}\PY{p}{,} \PY{n}{pos2}\PY{p}{,} \PY{n}{neg2}\PY{p}{)}\PY{p}{:}
             \PY{n}{pos}\PY{p}{,} \PY{n}{neg} \PY{o}{=} \PY{n}{pos1} \PY{o}{+} \PY{n}{neg1}\PY{p}{,} \PY{n}{pos2} \PY{o}{+} \PY{n}{neg2}
             \PY{n}{E\PYZus{}before} \PY{o}{=} \PY{n}{entropy}\PY{p}{(}\PY{n}{pos1}\PY{o}{+}\PY{n}{pos2}\PY{p}{,} \PY{n}{neg1}\PY{o}{+}\PY{n}{neg2}\PY{p}{)}
             \PY{n}{E\PYZus{}after} \PY{o}{=} \PY{p}{(}\PY{n}{pos} \PY{o}{*} \PY{n}{entropy}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{neg1}\PY{p}{)} \PY{o}{+} \PY{n}{neg} \PY{o}{*} \PY{n}{entropy}\PY{p}{(}\PY{n}{pos2}\PY{p}{,} \PY{n}{neg2}\PY{p}{)}\PY{p}{)}\PY{o}{/} \PY{p}{(}\PY{n}{pos}\PY{o}{+}\PY{n}{neg}\PY{p}{)}
             \PY{k}{return} \PY{n}{E\PYZus{}before} \PY{o}{\PYZhy{}} \PY{n}{E\PYZus{}after}
\end{Verbatim}

    \paragraph{What is the class entropy for the entire dataset? What is the
information gain when you split the data using the Water
feature?}\label{what-is-the-class-entropy-for-the-entire-dataset-what-is-the-information-gain-when-you-split-the-data-using-the-water-feature}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{pos1} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{neg1} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{E} \PY{o}{=} \PY{n}{entropy}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{neg1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} class entropy of entire dataset}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class entropy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{E}\PY{p}{)}
         
         \PY{n}{pos2} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water\PYZus{}warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{n}{neg2} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water\PYZus{}warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{n}{pos3} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water\PYZus{}warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{n}{neg3} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water\PYZus{}warm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{IG} \PY{o}{=} \PY{n}{info\PYZus{}gain}\PY{p}{(}\PY{n}{pos2}\PY{p}{,} \PY{n}{neg2}\PY{p}{,} \PY{n}{pos3}\PY{p}{,} \PY{n}{neg3}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Information gain:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{IG}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Class entropy: 0.970950594455
Information gain: 0.321928094887

    \end{Verbatim}

    \paragraph{Implement a basic decision
tree}\label{implement-a-basic-decision-tree}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{graphviz} \PY{k}{import} \PY{n}{Digraph}
         
         \PY{k}{def} \PY{n+nf}{create\PYZus{}tree}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{features}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{dot}\PY{o}{=}\PY{n}{Digraph}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{parent}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{sclass}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{k}{global} \PY{n}{i}
             \PY{k}{if} \PY{n}{features} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
                 \PY{n}{features} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{df}\PY{p}{)}
                 \PY{n}{features}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{max\PYZus{}IG}\PY{p}{,} \PY{n}{best\PYZus{}f} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
             \PY{n}{node\PYZus{}id} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}
             \PY{n}{i}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
             
             \PY{c+c1}{\PYZsh{} stop when count of either class is zero}
             \PY{n}{pos1} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{neg1} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             \PY{n}{E} \PY{o}{=} \PY{n}{entropy}\PY{p}{(}\PY{n}{pos1}\PY{p}{,} \PY{n}{neg1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} class entropy of entire dataset}
             \PY{k}{if} \PY{n}{E} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} display nodes}
                 \PY{n}{dot}\PY{o}{.}\PY{n}{node}\PY{p}{(}\PY{n}{node\PYZus{}id}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{] }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ class = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{neg1}\PY{p}{,} \PY{n}{pos1}\PY{p}{,} \PY{n}{sclass}\PY{p}{)}\PY{p}{)}
                 \PY{k}{if} \PY{n}{parent}\PY{p}{:}
                     \PY{n}{dot}\PY{o}{.}\PY{n}{edge}\PY{p}{(}\PY{n}{parent}\PY{p}{,} \PY{n}{node\PYZus{}id}\PY{p}{)}
                 \PY{k}{return}
         
             \PY{c+c1}{\PYZsh{} find the best splitting feature}
             \PY{k}{for} \PY{n}{f} \PY{o+ow}{in} \PY{n}{features}\PY{p}{:}
                 \PY{n}{pos2} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 \PY{n}{neg2} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 \PY{n}{pos3} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 \PY{n}{neg3} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 
                 \PY{n}{IG} \PY{o}{=} \PY{n}{info\PYZus{}gain}\PY{p}{(}\PY{n}{pos2}\PY{p}{,} \PY{n}{neg2}\PY{p}{,} \PY{n}{pos3}\PY{p}{,} \PY{n}{neg3}\PY{p}{)}
                 \PY{k}{if} \PY{n}{IG} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}IG}\PY{p}{:}
                     \PY{n}{max\PYZus{}IG} \PY{o}{=} \PY{n}{IG}
                     \PY{n}{best\PYZus{}f} \PY{o}{=} \PY{n}{f}
             
             \PY{c+c1}{\PYZsh{} stop if gain is zero}
             \PY{k}{if} \PY{n}{max\PYZus{}IG} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} display nodes}
                 \PY{n}{dot}\PY{o}{.}\PY{n}{node}\PY{p}{(}\PY{n}{node\PYZus{}id}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{] }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ class = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{neg1}\PY{p}{,} \PY{n}{pos1}\PY{p}{,} \PY{n}{sclass}\PY{p}{)}\PY{p}{)}
                 \PY{k}{if} \PY{n}{parent}\PY{p}{:}
                     \PY{n}{dot}\PY{o}{.}\PY{n}{edge}\PY{p}{(}\PY{n}{parent}\PY{p}{,} \PY{n}{node\PYZus{}id}\PY{p}{)}
                 \PY{k}{return}
             
             \PY{c+c1}{\PYZsh{} split on that feature | display nodes}
             \PY{n}{dot}\PY{o}{.}\PY{n}{node}\PY{p}{(}\PY{n}{node\PYZus{}id}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ value = [}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{] }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ class = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{best\PYZus{}f}\PY{p}{,} \PY{n}{neg1}\PY{p}{,} \PY{n}{pos1}\PY{p}{,} \PY{n}{sclass}\PY{p}{)}\PY{p}{)}
             \PY{k}{if} \PY{n}{parent}\PY{p}{:}
                 \PY{n}{dot}\PY{o}{.}\PY{n}{edge}\PY{p}{(}\PY{n}{parent}\PY{p}{,} \PY{n}{node\PYZus{}id}\PY{p}{)}
             \PY{n}{features}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{n}{best\PYZus{}f}\PY{p}{)}
             \PY{n}{create\PYZus{}tree}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{n}{best\PYZus{}f}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{dot}\PY{p}{,} \PY{n}{node\PYZus{}id}\PY{p}{,} \PY{n}{sclass}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{n}{create\PYZus{}tree}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{n}{best\PYZus{}f}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{dot}\PY{p}{,} \PY{n}{node\PYZus{}id}\PY{p}{,} \PY{n}{sclass}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{dot}
         
         \PY{c+c1}{\PYZsh{} Create the tree and display the graph}
         \PY{n}{i}\PY{o}{=}\PY{l+m+mi}{0}
         \PY{n}{g} \PY{o}{=} \PY{n}{Digraph}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Decision Tree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{node\PYZus{}attr}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rectangle}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{display}\PY{p}{(}\PY{n}{create\PYZus{}tree}\PY{p}{(}\PY{n}{df1}\PY{p}{,} \PY{n}{dot}\PY{o}{=}\PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_35_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Train a scikit-learn decision tree on the same
data}\label{train-a-scikit-learn-decision-tree-on-the-same-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{export\PYZus{}graphviz}
         \PY{k+kn}{import} \PY{n+nn}{graphviz}
         
         \PY{c+c1}{\PYZsh{} build a tree model}
         \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{df1}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{df1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date?\PYZus{}yes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{clf} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Creates a .dot file}
         \PY{n}{export\PYZus{}graphviz}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{out\PYZus{}file}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tree.dot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{class\PYZus{}names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                         \PY{n}{feature\PYZus{}names}\PY{o}{=}\PY{n+nb}{list}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{impurity}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{filled}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Open and display}
         \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tree.dot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
             \PY{n}{dot\PYZus{}graph} \PY{o}{=} \PY{n}{f}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}
         \PY{n}{display}\PY{p}{(}\PY{n}{graphviz}\PY{o}{.}\PY{n}{Source}\PY{p}{(}\PY{n}{dot\PYZus{}graph}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_37_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see clearly that the results from both decision trees are the
same.

    \subsection{Random Forests (4 points
(1+1+2))}\label{random-forests-4-points-112}

Study the effect of the number of trees in a RandomForest on the
EEG-eye-state dataset (http://www.openml.org/d/1471). This dataset
measures brain activity using 15 sensors, and you need to predict
whether the person's eyes are open or closed.

\begin{itemize}
\tightlist
\item
  Train a RandomForest classifier on this dataset with an increasing
  number of trees (on a log scale as above). Plot the Out-Of-Bag error
  against the number of trees.

  \begin{itemize}
  \tightlist
  \item
    The Out-Of-Bag error is the test error obtained when using
    bootstrapping, and using the non-drawn data points as the test set.
    This is what a RandomForest does internally, so you can retrieve it
    from the classifier. The code below hints on how to do this.
  \end{itemize}
\item
  Construct the same plot, but now use 10-fold Cross-validation and
  error rate instead of the OOB error. Compare the two. What do you
  learn from this?
\item
  Compare the performance of the RandomForest ensemble with that of a
  single full decision tree. Compute the AUC as well as the bias and
  variance. Does the bias and variance increase/decrease for the
  ensemble? Does the number of trees affect the result?
\end{itemize}

Hint: Error rate = 1 - accuracy. It is not a standard scoring metric for
cross\_val\_score, so you'll need to let it compute the accuracy values,
and then compute the mean error rate yourself.\\
Hint: We discussed bias-variance decomposition in class. It is not
included in scikit-learn, so you'll need to implement it yourself.
Always first calculate the bias and variance of each sample
individually, and then sum them up.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{ensemble}
        \PY{n}{eeg} \PY{o}{=} \PY{n}{oml}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{l+m+mi}{1471}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Download Ionosphere data}
        \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{eeg}\PY{o}{.}\PY{n}{get\PYZus{}data}\PY{p}{(}\PY{n}{target}\PY{o}{=}\PY{n}{eeg}\PY{o}{.}\PY{n}{default\PYZus{}target\PYZus{}attribute}\PY{p}{)}
\end{Verbatim}

    \paragraph{Plot the Out-Of-Bag error against the number of
trees.}\label{plot-the-out-of-bag-error-against-the-number-of-trees.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} Out of bag errors can be retrieved from the RandomForest classifier. You\PYZsq{}ll need to loop over the number of trees.}
         \PY{c+c1}{\PYZsh{} http://scikit\PYZhy{}learn.org/stable/auto\PYZus{}examples/ensemble/plot\PYZus{}ensemble\PYZus{}oob.html}
         \PY{n}{clf} \PY{o}{=} \PY{n}{ensemble}\PY{o}{.}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{warm\PYZus{}start}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{oob\PYZus{}score}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]}
         \PY{n}{results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{n\PYZus{}estimators}\PY{p}{:}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{n}{i}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
             \PY{n}{results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{clf}\PY{o}{.}\PY{n}{oob\PYZus{}score\PYZus{}}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} plot the data}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{,} \PY{n}{results}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{OOB error rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} [<matplotlib.lines.Line2D at 0x1e3ffd7ae80>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_42_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Construct the same plot, but now use 10-fold Cross-validation
and error rate instead of the OOB
error.}\label{construct-the-same-plot-but-now-use-10-fold-cross-validation-and-error-rate-instead-of-the-oob-error.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{ensemble}\PY{o}{.}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]}
         \PY{n}{results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{n\PYZus{}estimators}\PY{p}{:}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{n}{i}\PY{p}{)}
             \PY{n}{scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{scores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} plot the data}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{,} \PY{n}{results}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}26}]:} [<matplotlib.lines.Line2D at 0x1e3f5c3b668>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_44_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    Both of them share the same trend, while the OOB error is much smaller
than the CV error rate for fixed number of estimators. Comparing to the
CV error, the OOB error plot is smoother. This is because the OOB error
is taken as the average of the all estimated errors as the forest grows.
Therefore OOB should be taken as the measure.

    \paragraph{Compare the performance of the RandomForest ensemble with
that of a single full decision
tree.}\label{compare-the-performance-of-the-randomforest-ensemble-with-that-of-a-single-full-decision-tree.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{,} \PY{n}{ShuffleSplit}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}auc\PYZus{}score}
         
         \PY{n}{n\PYZus{}repeat} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{ss} \PY{o}{=} \PY{n}{ShuffleSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{n}{n\PYZus{}repeat}\PY{p}{,} \PY{n}{train\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.66}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{ensemble}\PY{o}{.}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]}\PY{o}{+}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bias}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{\PYZcb{}}
         
         \PY{k}{def} \PY{n+nf}{bias}\PY{p}{(}\PY{n}{x\PYZus{}true}\PY{p}{,} \PY{n}{x\PYZus{}predicted}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}predicted}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{n}{x\PYZus{}true}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}predicted}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{k}{def} \PY{n+nf}{variance}\PY{p}{(}\PY{n}{x\PYZus{}predicted}\PY{p}{)}\PY{p}{:}
             \PY{n}{P\PYZus{}class\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x\PYZus{}predicted}\PY{p}{)}
             \PY{n}{P\PYZus{}class\PYZus{}0} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{P\PYZus{}class\PYZus{}1}
             \PY{k}{return} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{p}{(}\PY{n}{P\PYZus{}class\PYZus{}1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{p}{(}\PY{n}{P\PYZus{}class\PYZus{}0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{/} \PY{l+m+mi}{2}
         
         \PY{k}{for} \PY{n}{c}\PY{p}{,} \PY{n}{i}  \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{:}
             \PY{n}{predictions} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{p}{]} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{]}
             \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{clf} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{set\PYZus{}params}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{n}{i}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{train\PYZus{}index}\PY{p}{,} \PY{n}{test\PYZus{}index} \PY{o+ow}{in} \PY{n}{ss}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Train and predict}
                 \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}\PY{p}{)}
                 \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}index}\PY{p}{]}\PY{p}{)}
                 \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc\PYZus{}score}\PY{p}{(}\PY{n}{y}\PY{p}{[}\PY{n}{test\PYZus{}index}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} Store predictions}
                 \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{index} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}index}\PY{p}{)}\PY{p}{:}
                     \PY{n}{predictions}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{bias\PYZus{}sq} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{n}{bias}\PY{p}{(}\PY{n}{y}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{/} \PY{n}{n\PYZus{}repeat}\PY{p}{)} \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}\PY{p}{]}\PY{p}{)}
             \PY{n}{var} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{[}\PY{n}{variance}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{/} \PY{n}{n\PYZus{}repeat}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{predictions}\PY{p}{]}\PY{p}{)}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bias}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bias\PYZus{}sq}\PY{p}{)}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{var}\PY{p}{)}
             \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bias}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{c}\PY{p}{]}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{c}\PY{p}{]}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{c}\PY{p}{]}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} plot the data}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bias}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bias}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{variance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{roc\PYZus{}auc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n\PYZus{}estimators}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
135.198794955701 194.681205044 0.777363407561
173.45079915396448 145.199200846 0.770241303299
124.13527551553285 114.634724484 0.829863080466
98.4649823162248 83.2750176838 0.871913468914
88.54253331269976 59.4574666873 0.896368635433
86.4286539707653 43.8213460292 0.909118249689
85.875076187572 33.9249238124 0.91656016442
86.89602553062683 27.9339744694 0.920093762152
88.16297071057593 24.2770292894 0.921818292825
89.28486197681735 21.9051380232 0.922749457418
89.88569631306076 20.8343036869 0.923076251838
109.86830496592835 151.211695034 0.824075565527

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} [<matplotlib.lines.Line2D at 0x24cb34b1dd8>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_47_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} [<matplotlib.lines.Line2D at 0x24ca6fc99b0>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_47_4.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} [<matplotlib.lines.Line2D at 0x24cb364eeb8>]
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_47_6.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    The \(n\_estimators=-1\) simply corresponds to a normal
DecisionTreeClassifier as can seen from the code also. As the number of
estimators grows, the bias and variance of the RF model decrees
significantly became much smaller than of the single decision tree. As
for the AUC score, the RF model outperforms single decision tree as nr-
estimators grows

    \subsection{A regression benchmark (1
point)}\label{a-regression-benchmark-1-point}

Consider the liver-disorder dataset (http://www.openml.org/d/8). The
goal is to predict how much alcohol someone consumed based on blood test
values.

\begin{itemize}
\tightlist
\item
  Take a selection of the algorithms that we covered in class that can
  do regression.
\item
  Based on what you learned in the previous exercises, make educated
  guesses about good hyperparameter values and set up a grid or random
  search.
\item
  Evaluate all models with 10-fold cross-validation and root mean
  squared error (RMSE). Report all results. Which model yields the best
  results?
\end{itemize}

Hint: mean squared error (MSE) is a standard scoring technique in
GridSearchCV and cross\_val\_score. You'll have to compute the square
roots yourself. Of course, during a grid search you can just use MSE,
the optimal hyperparameter values will be the same.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{liver} \PY{o}{=} \PY{n}{oml}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Download Liver\PYZhy{}disorders data}
         \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{liver}\PY{o}{.}\PY{n}{get\PYZus{}data}\PY{p}{(}\PY{n}{target}\PY{o}{=}\PY{n}{liver}\PY{o}{.}\PY{n}{default\PYZus{}target\PYZus{}attribute}\PY{p}{)}
         \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{shape}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mglearn}\PY{o}{.}\PY{n}{discrete\PYZus{}scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} ((345, 5), (345,))
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{A1-Iyer-Chen_files/A1-Iyer-Chen_50_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Take a selection of the algorithms that we covered in class
that can do regression. Make educated guesses about good hyperparameter
values.}\label{take-a-selection-of-the-algorithms-that-we-covered-in-class-that-can-do-regression.-make-educated-guesses-about-good-hyperparameter-values.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}\PY{p}{,} \PY{n}{Ridge}\PY{p}{,} \PY{n}{Lasso}\PY{p}{,} \PY{n}{ElasticNet}\PY{p}{,} \PY{n}{SGDRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeRegressor}
         
         \PY{n}{estimators} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{knn}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{KNeighborsRegressor}\PY{p}{(}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{LinearRegression}\PY{p}{(}\PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Ridge}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{Lasso}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{elasticnet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ElasticNet}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                       \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{decision}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{DecisionTreeRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{66}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}               (\PYZsq{}sgd\PYZsq{}, SGDRegressor(n\PYZus{}iter=30000))]}
         
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{knn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{51}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}iter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{10000}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{elasticnet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1\PYZus{}ratio}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.25}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.75}\PY{p}{,} \PY{l+m+mf}{0.95}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}iter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{10000}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     \PYZsq{}sgd\PYZsq{}: \PYZob{}}
         \PY{c+c1}{\PYZsh{}         \PYZsq{}penalty\PYZsq{}: [\PYZsq{}l2\PYZsq{}, \PYZsq{}l1\PYZsq{}, \PYZsq{}elasticnet\PYZsq{}],}
         \PY{c+c1}{\PYZsh{}         \PYZsq{}alpha\PYZsq{}: [10, 100, 1000, 10000, 100000],\PYZsh{}0.001, 0.01, 0.1, 1, }
         \PY{c+c1}{\PYZsh{}         \PYZsq{}l1\PYZus{}ratio\PYZsq{}: [0.05, 0.25, 0.5, 0.65, 0.75, 0.95, 1]}
         \PY{c+c1}{\PYZsh{}     \PYZcb{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{decision}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}
         \PY{p}{\PYZcb{}}
\end{Verbatim}

    \paragraph{Evaluate all models with 10-fold cross-validation and root
mean squared error
(RMSE).}\label{evaluate-all-models-with-10-fold-cross-validation-and-root-mean-squared-error-rmse.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         
         \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{clf} \PY{o+ow}{in} \PY{n}{estimators}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ \PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{key}\PY{p}{)}
             \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Get the RMSE}
             \PY{n}{scores} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{*}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{scores\PYZus{}with\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{params}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{scores}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scores:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{scores\PYZus{}with\PYZus{}params}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{+\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\#\#\#\#\# knn \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'n\_neighbors': 1\}, 4.6718397828394904),
          (\{'n\_neighbors': 2\}, 3.9625054290640813),
          (\{'n\_neighbors': 3\}, 3.8609069212334477),
          (\{'n\_neighbors': 4\}, 3.826987501068345),
          (\{'n\_neighbors': 5\}, 3.7535374619498434),
          (\{'n\_neighbors': 6\}, 3.6884836224622584),
          (\{'n\_neighbors': 7\}, 3.6456876106424962),
          (\{'n\_neighbors': 8\}, 3.5867042621002523),
          (\{'n\_neighbors': 9\}, 3.5494412000129341),
          (\{'n\_neighbors': 10\}, 3.51942436004625),
          (\{'n\_neighbors': 11\}, 3.5437616308925772),
          (\{'n\_neighbors': 12\}, 3.5179605880215297),
          (\{'n\_neighbors': 13\}, 3.5083640734435151),
          (\{'n\_neighbors': 14\}, 3.5361550010794596),
          (\{'n\_neighbors': 15\}, 3.5437109473802924),
          (\{'n\_neighbors': 16\}, 3.5329213211723527),
          (\{'n\_neighbors': 17\}, 3.5394826479833688),
          (\{'n\_neighbors': 18\}, 3.5266373827202897),
          (\{'n\_neighbors': 19\}, 3.5459079805884355),
          (\{'n\_neighbors': 20\}, 3.5381000796915067),
          (\{'n\_neighbors': 21\}, 3.5371318492183939),
          (\{'n\_neighbors': 22\}, 3.5335642313091804),
          (\{'n\_neighbors': 23\}, 3.5324868865626216),
          (\{'n\_neighbors': 24\}, 3.5318185044634176),
          (\{'n\_neighbors': 25\}, 3.5168075114318911),
          (\{'n\_neighbors': 26\}, 3.5234224295440333),
          (\{'n\_neighbors': 27\}, 3.5250091276636897),
          (\{'n\_neighbors': 28\}, 3.5315642666914977),
          (\{'n\_neighbors': 29\}, 3.5284575843521484),
          (\{'n\_neighbors': 30\}, 3.5268060208260144),
          (\{'n\_neighbors': 31\}, 3.5267152379776698),
          (\{'n\_neighbors': 32\}, 3.5279350703360808),
          (\{'n\_neighbors': 33\}, 3.5257321867835962),
          (\{'n\_neighbors': 34\}, 3.5271401470554102),
          (\{'n\_neighbors': 35\}, 3.5331735020464876),
          (\{'n\_neighbors': 36\}, 3.5341280160430859),
          (\{'n\_neighbors': 37\}, 3.5297548629698645),
          (\{'n\_neighbors': 38\}, 3.5271228740430485),
          (\{'n\_neighbors': 39\}, 3.531601124178684),
          (\{'n\_neighbors': 40\}, 3.5318927368123489),
          (\{'n\_neighbors': 41\}, 3.5310696740451002),
          (\{'n\_neighbors': 42\}, 3.5296395731178403),
          (\{'n\_neighbors': 43\}, 3.5237484930850087),
          (\{'n\_neighbors': 44\}, 3.5276483965901799),
          (\{'n\_neighbors': 45\}, 3.5282790291439352),
          (\{'n\_neighbors': 46\}, 3.5297407290806939),
          (\{'n\_neighbors': 47\}, 3.5295789711200443),
          (\{'n\_neighbors': 48\}, 3.5290144138972233),
          (\{'n\_neighbors': 49\}, 3.5301921574352217),
          (\{'n\_neighbors': 50\}, 3.5300451202108785)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.58539481065 +- 0.0325845161212
\#\#\#\#\# linear \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{\}, 3.4456431153534575)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.44564311535 +- 0.0
\#\#\#\#\# ridge \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'alpha': 0.001\}, 3.4456429541410283),
          (\{'alpha': 0.01\}, 3.4456429901954428),
          (\{'alpha': 0.1\}, 3.4456433507597781),
          (\{'alpha': 1\}, 3.4456469584214031),
          (\{'alpha': 10\}, 3.44568323603474),
          (\{'alpha': 100\}, 3.4460653050861536)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.44572079911 +- 2.3943352142e-08
\#\#\#\#\# lasso \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'alpha': 0.001, 'max\_iter': 10000\}, 3.4456520755314606),
          (\{'alpha': 0.001, 'max\_iter': 1000\}, 3.4456520755314606),
          (\{'alpha': 0.001, 'max\_iter': 100\}, 3.4456520755314606),
          (\{'alpha': 0.01, 'max\_iter': 10000\}, 3.4457314402092551),
          (\{'alpha': 0.01, 'max\_iter': 1000\}, 3.4457314402092551),
          (\{'alpha': 0.01, 'max\_iter': 100\}, 3.4457314402092551),
          (\{'alpha': 0.1, 'max\_iter': 10000\}, 3.4466972827693763),
          (\{'alpha': 0.1, 'max\_iter': 1000\}, 3.4466972827693763),
          (\{'alpha': 0.1, 'max\_iter': 100\}, 3.4466972827693763),
          (\{'alpha': 1, 'max\_iter': 10000\}, 3.4630936276346374),
          (\{'alpha': 1, 'max\_iter': 1000\}, 3.4630936276346374),
          (\{'alpha': 1, 'max\_iter': 100\}, 3.4630936276346374),
          (\{'alpha': 10, 'max\_iter': 10000\}, 3.5743704593953392),
          (\{'alpha': 10, 'max\_iter': 1000\}, 3.5743704593953392),
          (\{'alpha': 10, 'max\_iter': 100\}, 3.5743704593953392),
          (\{'alpha': 100, 'max\_iter': 10000\}, 3.7276453159068561),
          (\{'alpha': 100, 'max\_iter': 1000\}, 3.7276453159068561),
          (\{'alpha': 100, 'max\_iter': 100\}, 3.7276453159068561)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.51719836691 +- 0.0109467809334
\#\#\#\#\# elasticnet \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'alpha': 0.001, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.4456450472610993),
          (\{'alpha': 0.001, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.4456450472610993),
          (\{'alpha': 0.001, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.4456447414034912),
          (\{'alpha': 0.001, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.4456462971347173),
          (\{'alpha': 0.001, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.4456462971347173),
          (\{'alpha': 0.001, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.4456462971347173),
          (\{'alpha': 0.001, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.4456482185289428),
          (\{'alpha': 0.001, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.4456482185289428),
          (\{'alpha': 0.001, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.4456482185289428),
          (\{'alpha': 0.001, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.4456502169017842),
          (\{'alpha': 0.001, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.4456502169017842),
          (\{'alpha': 0.001, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.4456502169017842),
          (\{'alpha': 0.001, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.4456517244213303),
          (\{'alpha': 0.001, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.4456517244213303),
          (\{'alpha': 0.001, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.4456517244213303),
          (\{'alpha': 0.001, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.4456520755314606),
          (\{'alpha': 0.001, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.4456520755314606),
          (\{'alpha': 0.001, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.4456520755314606),
          (\{'alpha': 0.01, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.4456594560271845),
          (\{'alpha': 0.01, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.4456594560271845),
          (\{'alpha': 0.01, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.4456594560271845),
          (\{'alpha': 0.01, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.4456744400265733),
          (\{'alpha': 0.01, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.4456744400265733),
          (\{'alpha': 0.01, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.4456744400265733),
          (\{'alpha': 0.01, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.4456935748379318),
          (\{'alpha': 0.01, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.4456935748379318),
          (\{'alpha': 0.01, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.4456935748379318),
          (\{'alpha': 0.01, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.4457139971903579),
          (\{'alpha': 0.01, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.4457139971903579),
          (\{'alpha': 0.01, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.4457139971903579),
          (\{'alpha': 0.01, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.4457274226921419),
          (\{'alpha': 0.01, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.4457274226921419),
          (\{'alpha': 0.01, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.4457274226921419),
          (\{'alpha': 0.01, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.4457314402092551),
          (\{'alpha': 0.01, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.4457314402092551),
          (\{'alpha': 0.01, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.4457314402092551),
          (\{'alpha': 0.1, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.4458089514240506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.4458089514240506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.4458089514240506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.445971738339658),
          (\{'alpha': 0.1, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.445971738339658),
          (\{'alpha': 0.1, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.445971738339658),
          (\{'alpha': 0.1, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.4461981583384893),
          (\{'alpha': 0.1, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.4461981583384893),
          (\{'alpha': 0.1, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.4461981583384893),
          (\{'alpha': 0.1, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.4464407651125506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.4464407651125506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.4464407651125506),
          (\{'alpha': 0.1, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.4466444294060548),
          (\{'alpha': 0.1, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.4466444294060548),
          (\{'alpha': 0.1, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.4466444294060548),
          (\{'alpha': 0.1, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.4466972827693763),
          (\{'alpha': 0.1, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.4466972827693763),
          (\{'alpha': 0.1, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.4466972827693763),
          (\{'alpha': 1, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.4476072397597992),
          (\{'alpha': 1, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.4476072397597992),
          (\{'alpha': 1, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.4476072397597992),
          (\{'alpha': 1, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.4503664441943416),
          (\{'alpha': 1, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.4503664441943416),
          (\{'alpha': 1, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.4503664441943416),
          (\{'alpha': 1, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.4548854637882185),
          (\{'alpha': 1, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.4548854637882185),
          (\{'alpha': 1, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.4548854637882185),
          (\{'alpha': 1, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.4589898375252699),
          (\{'alpha': 1, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.4589898375252699),
          (\{'alpha': 1, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.4589898375252699),
          (\{'alpha': 1, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.4621924601253578),
          (\{'alpha': 1, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.4621924601253578),
          (\{'alpha': 1, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.4621924601253578),
          (\{'alpha': 1, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.4630936276346374),
          (\{'alpha': 1, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.4630936276346374),
          (\{'alpha': 1, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.4630936276346374),
          (\{'alpha': 10, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.4755829584152114),
          (\{'alpha': 10, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.4755829584152114),
          (\{'alpha': 10, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.4755829584152114),
          (\{'alpha': 10, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.5366428681956052),
          (\{'alpha': 10, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.5366428681956052),
          (\{'alpha': 10, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.5366428681956052),
          (\{'alpha': 10, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.5618522899536713),
          (\{'alpha': 10, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.5618522899536713),
          (\{'alpha': 10, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.5618522899536713),
          (\{'alpha': 10, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.5665982268973235),
          (\{'alpha': 10, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.5665982268973235),
          (\{'alpha': 10, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.5665982268973235),
          (\{'alpha': 10, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.5727106495301242),
          (\{'alpha': 10, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.5727106495301242),
          (\{'alpha': 10, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.5727106495301242),
          (\{'alpha': 10, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.5743704593953392),
          (\{'alpha': 10, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.5743704593953392),
          (\{'alpha': 10, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.5743704593953392),
          (\{'alpha': 100, 'l1\_ratio': 0.05, 'max\_iter': 10000\}, 3.5634172160326232),
          (\{'alpha': 100, 'l1\_ratio': 0.05, 'max\_iter': 1000\}, 3.5634172160326232),
          (\{'alpha': 100, 'l1\_ratio': 0.05, 'max\_iter': 100\}, 3.5634172160326232),
          (\{'alpha': 100, 'l1\_ratio': 0.25, 'max\_iter': 10000\}, 3.6512253843827174),
          (\{'alpha': 100, 'l1\_ratio': 0.25, 'max\_iter': 1000\}, 3.6512253843827174),
          (\{'alpha': 100, 'l1\_ratio': 0.25, 'max\_iter': 100\}, 3.6512253843827174),
          (\{'alpha': 100, 'l1\_ratio': 0.5, 'max\_iter': 10000\}, 3.7281971036444057),
          (\{'alpha': 100, 'l1\_ratio': 0.5, 'max\_iter': 1000\}, 3.7281971036444057),
          (\{'alpha': 100, 'l1\_ratio': 0.5, 'max\_iter': 100\}, 3.7281971036444057),
          (\{'alpha': 100, 'l1\_ratio': 0.75, 'max\_iter': 10000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 0.75, 'max\_iter': 1000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 0.75, 'max\_iter': 100\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 0.95, 'max\_iter': 10000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 0.95, 'max\_iter': 1000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 0.95, 'max\_iter': 100\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 1, 'max\_iter': 10000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 1, 'max\_iter': 1000\}, 3.7276453159068561),
          (\{'alpha': 100, 'l1\_ratio': 1, 'max\_iter': 100\}, 3.7276453159068561)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.50490342532 +- 0.0088617594994
\#\#\#\#\# rf \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'max\_features': 1, 'n\_estimators': 1\}, 4.548291604738985),
          (\{'max\_features': 1, 'n\_estimators': 2\}, 3.7984550635179586),
          (\{'max\_features': 1, 'n\_estimators': 4\}, 3.6630499290357337),
          (\{'max\_features': 1, 'n\_estimators': 8\}, 3.6955773116884818),
          (\{'max\_features': 1, 'n\_estimators': 16\}, 3.4722600239488175),
          (\{'max\_features': 1, 'n\_estimators': 32\}, 3.4398785413919755),
          (\{'max\_features': 1, 'n\_estimators': 64\}, 3.4076287762624018),
          (\{'max\_features': 1, 'n\_estimators': 128\}, 3.3743953996668163),
          (\{'max\_features': 1, 'n\_estimators': 256\}, 3.3596206818971868),
          (\{'max\_features': 1, 'n\_estimators': 512\}, 3.3722451269895695),
          (\{'max\_features': 1, 'n\_estimators': 1024\}, 3.3678463463711039),
          (\{'max\_features': 1, 'n\_estimators': 2048\}, 3.3671248283237616),
          (\{'max\_features': 2, 'n\_estimators': 1\}, 4.3744979007947835),
          (\{'max\_features': 2, 'n\_estimators': 2\}, 3.9378321115977553),
          (\{'max\_features': 2, 'n\_estimators': 4\}, 3.6660160819401049),
          (\{'max\_features': 2, 'n\_estimators': 8\}, 3.5991960152958797),
          (\{'max\_features': 2, 'n\_estimators': 16\}, 3.5057293298339465),
          (\{'max\_features': 2, 'n\_estimators': 32\}, 3.4764684332250373),
          (\{'max\_features': 2, 'n\_estimators': 64\}, 3.4518075516519318),
          (\{'max\_features': 2, 'n\_estimators': 128\}, 3.4200848069334255),
          (\{'max\_features': 2, 'n\_estimators': 256\}, 3.4184291615371163),
          (\{'max\_features': 2, 'n\_estimators': 512\}, 3.4135205203366339),
          (\{'max\_features': 2, 'n\_estimators': 1024\}, 3.4147745746247131),
          (\{'max\_features': 2, 'n\_estimators': 2048\}, 3.4088097997437226),
          (\{'max\_features': 3, 'n\_estimators': 1\}, 4.3933414703801841),
          (\{'max\_features': 3, 'n\_estimators': 2\}, 3.9879347021241203),
          (\{'max\_features': 3, 'n\_estimators': 4\}, 3.8223456263080817),
          (\{'max\_features': 3, 'n\_estimators': 8\}, 3.6626666257788467),
          (\{'max\_features': 3, 'n\_estimators': 16\}, 3.5811724610317461),
          (\{'max\_features': 3, 'n\_estimators': 32\}, 3.4903893996561308),
          (\{'max\_features': 3, 'n\_estimators': 64\}, 3.4622447037743314),
          (\{'max\_features': 3, 'n\_estimators': 128\}, 3.4514790592505045),
          (\{'max\_features': 3, 'n\_estimators': 256\}, 3.4467242708964596),
          (\{'max\_features': 3, 'n\_estimators': 512\}, 3.4346004885684098),
          (\{'max\_features': 3, 'n\_estimators': 1024\}, 3.4390364149263686),
          (\{'max\_features': 3, 'n\_estimators': 2048\}, 3.4339381059226022),
          (\{'max\_features': 4, 'n\_estimators': 1\}, 4.4818474451657915),
          (\{'max\_features': 4, 'n\_estimators': 2\}, 4.0591639033430145),
          (\{'max\_features': 4, 'n\_estimators': 4\}, 3.686073477449876),
          (\{'max\_features': 4, 'n\_estimators': 8\}, 3.5755181037361461),
          (\{'max\_features': 4, 'n\_estimators': 16\}, 3.5483127113920059),
          (\{'max\_features': 4, 'n\_estimators': 32\}, 3.5256171407040338),
          (\{'max\_features': 4, 'n\_estimators': 64\}, 3.5005618321708658),
          (\{'max\_features': 4, 'n\_estimators': 128\}, 3.4809148023452963),
          (\{'max\_features': 4, 'n\_estimators': 256\}, 3.4770158651428291),
          (\{'max\_features': 4, 'n\_estimators': 512\}, 3.4611647628840525),
          (\{'max\_features': 4, 'n\_estimators': 1024\}, 3.4684154699327103),
          (\{'max\_features': 4, 'n\_estimators': 2048\}, 3.4637407578304562),
          (\{'max\_features': 5, 'n\_estimators': 1\}, 4.7067635577332538),
          (\{'max\_features': 5, 'n\_estimators': 2\}, 4.0150080765886962),
          (\{'max\_features': 5, 'n\_estimators': 4\}, 3.6935117416230381),
          (\{'max\_features': 5, 'n\_estimators': 8\}, 3.580461015980557),
          (\{'max\_features': 5, 'n\_estimators': 16\}, 3.5439671769712082),
          (\{'max\_features': 5, 'n\_estimators': 32\}, 3.5171789037697794),
          (\{'max\_features': 5, 'n\_estimators': 64\}, 3.4978281426269731),
          (\{'max\_features': 5, 'n\_estimators': 128\}, 3.4814112905533441),
          (\{'max\_features': 5, 'n\_estimators': 256\}, 3.4901823796668179),
          (\{'max\_features': 5, 'n\_estimators': 512\}, 3.4841913573646859),
          (\{'max\_features': 5, 'n\_estimators': 1024\}, 3.4958285582734954),
          (\{'max\_features': 5, 'n\_estimators': 2048\}, 3.4906089969932541)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 3.62141167867 +- 0.0964402772581
\#\#\#\#\# decision \#\#\#\#\#
Scores:

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} [(\{'max\_features': 1\}, 4.7800915642191253),
          (\{'max\_features': 2\}, 4.6603476887148823),
          (\{'max\_features': 3\}, 4.7539457296018854),
          (\{'max\_features': 4\}, 4.8060951639181768),
          (\{'max\_features': 5\}, 4.5342496942553874)]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
Mean score: 4.70694596814 +- 0.00987704429714

    \end{Verbatim}

    We stopped testing with SGD regressor because it took too much time to
train and also produced very poor results. Overall, Linear regression
and Ridge regression scored the same and the best. ElasticNet and Lasso
regression models produced the same \(RMSE=3.446\) as Linear and Ridge
regression for some combination of the parameters but got progressively
worse for other combinations especially from increase in alpha (greater
regularization). You can see for which combinations the models produced
the best result. Random Forests Regression even produced
\(RMSE \leq 3.446\) when \(n\_estimators\) was sufficiently large and
\(max\_features\) generally low but on average was worse. Lastly, KNN
regression and Decision tree regression produced relatively bad results.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
